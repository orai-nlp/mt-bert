# Data

Pretraining corpora employed in this work:

* [N_ElhBERTeu](https://storage.googleapis.com/elhuyar/mtbert/data/ElhBERTeu.txt.gz)
* [N_small](https://storage.googleapis.com/elhuyar/mtbert/data/eu_125M.txt.gz)
* [S_beto2eu](https://storage.googleapis.com/elhuyar/mtbert/data/S_beto2eu.txt.gz)
* [S_loc2eu](https://storage.googleapis.com/elhuyar/mtbert/data/S_loc2eu.txt.gz)
* [paralEU](https://storage.googleapis.com/elhuyar/mtbert/data/paral_eu.txt.gz)
* [paralES2EU](https://storage.googleapis.com/elhuyar/mtbert/data/paral_es2eu.txt.gz)

MLM evaluation dataset:
* [mlm_test](https://storage.googleapis.com/elhuyar/mtbert/data/berria_2021.txt.gz)

Downstream task evaluation benchmark:
* [BasqueGLUE](https://huggingface.co/datasets/orai-nlp/basqueGLUE)
