# Models

Models from our work Not Enough Data to Pre-train Your Language Model? MT to the Rescue! accepted at ACL2023 Findings.

Models
* [ElhBERTeu]() also in [HFðŸ¤—](https://huggingface.co/orai-nlp/ElhBERTeu)
* [BERT_125M]()
* [S_BERT]()
* [SN_BERT]()
* [Sloc_BERT]()
* [SNloc_BERT]()
* [paral_eu]()
* [paral_es2eu]()
* [concat_50-50]()
* [concat_80-20]()
* [seq]()
