# Models

Download links for our models from our work Not Enough Data to Pre-train Your Language Model? MT to the Rescue! accepted at ACL2023 Findings.

Models:
* [ElhBERTeu](https://storage.googleapis.com/elhuyar/mtbert/models/ElhBERTeu.tar.gz) or [HFðŸ¤—](https://huggingface.co/orai-nlp/ElhBERTeu) (Urbizu et al., 2022)
* [BERT_125M](https://storage.googleapis.com/elhuyar/mtbert/models/bert_125M.tar.gz)
* [S_BERT](https://storage.googleapis.com/elhuyar/mtbert/models/S_BERT.tar.gz)
* [SN_BERT](https://storage.googleapis.com/elhuyar/mtbert/models/SN_BERT.tar.gz)
* [Sloc_BERT](https://storage.googleapis.com/elhuyar/mtbert/models/Sloc_BERT.tar.gz)
* [SNloc_BERT](https://storage.googleapis.com/elhuyar/mtbert/models/SNloc_BERT.tar.gz)
* [paral_eu](https://storage.googleapis.com/elhuyar/mtbert/models/paral_EU.tar.gz)
* [paral_es2eu](https://storage.googleapis.com/elhuyar/mtbert/models/paral_ES2EU.tar.gz)
* [concat_50-50](https://storage.googleapis.com/elhuyar/mtbert/models/concat50-50.tar.gz)
* [concat_80-20](https://storage.googleapis.com/elhuyar/mtbert/models/concat80-20.tar.gz)
* [sequential](https://storage.googleapis.com/elhuyar/mtbert/models/sequential.tar.gz)


References

G. Urbizu, I. San Vicente, X. Saralegi, R. Agerri, A. Soroa. BasqueGLUE: A Natural Language Understanding Benchmark for Basque. In proceedings of the 13th Language Resources and Evaluation Conference (LREC 2022). June, 2022. Marseille, France
