# Models

Models from our work Not Enough Data to Pre-train Your Language Model? MT to the Rescue! accepted at ACL2023 Findings.

Models
* [ElhBERTeu](https://storage.googleapis.com/elhuyar/mtbert/models/ElhBERTeu.tar.gz) ([HFðŸ¤—](https://huggingface.co/orai-nlp/ElhBERTeu))
* [BERT_125M](https://storage.googleapis.com/elhuyar/mtbert/models/bert_125M.tar.gz)
* [S_BERT](https://storage.googleapis.com/elhuyar/mtbert/models/S_BERT.tar.gz)
* [SN_BERT](https://storage.googleapis.com/elhuyar/mtbert/models/SN_BERT.tar.gz)
* [Sloc_BERT](https://storage.googleapis.com/elhuyar/mtbert/models/Sloc_BERT.tar.gz)
* [SNloc_BERT](https://storage.googleapis.com/elhuyar/mtbert/models/SNloc_BERT.tar.gz)
* [paral_eu](https://storage.googleapis.com/elhuyar/mtbert/models/paral_EU.tar.gz)
* [paral_es2eu](https://storage.googleapis.com/elhuyar/mtbert/models/paral_ES2EU.tar.gz)
* [concat_50-50](https://storage.googleapis.com/elhuyar/mtbert/models/concat50-50.tar.gz)
* [concat_80-20](https://storage.googleapis.com/elhuyar/mtbert/models/concat80-20.tar.gz)
* [sequential](https://storage.googleapis.com/elhuyar/mtbert/models/sequential.tar.gz)
