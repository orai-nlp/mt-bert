## Not Enough Data to Pre-train Your Language Model? MT to the Rescue!

Data and models from our work *Not Enough Data to Pre-train Your Language Model? MT to the Rescue!* accepted at ACL2023 Findings.


Authors
-----------
Gorka Urbizu, Iñaki San Vicente, Xabier Saralegi, Ander Corral

Orai NLP Technologies



Licensing
-------------

Copyright (C) by Orai NLP Technologies. 
The corpora, datasets and models created in this work, are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
International License (CC BY-NC-SA 4.0). To view a copy of this license, visit [http://creativecommons.org/licenses/by-nc-sa/4.0/](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.eu).




Acknowledgements
-------------------
If you use any of these models or datasets please cite the following paper:

- G. Urbizu, I. San Vicente, X. Saralegi, A. Corral. Not Enough Data to Pre-train Your Language Model? MT to the Rescue! Findings of the Association for Computational Linguistics: ACL 2023. July, 2023. Toronto, Canada



Contact information
-----------------------
Gorka Urbizu, Iñaki San Vicente: {g.urbizu,i.sanvicente}@orai.eus
