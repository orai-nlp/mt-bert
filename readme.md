# Not Enough Data to Pre-train Your Language Model? MT to the Rescue!

Data (corpora and datasets) and models from our work *Not Enough Data to Pre-train Your Language Model? MT to the Rescue!* accepted at ACL2023 Findings.


Authors
-----------
Gorka Urbizu, Iñaki San Vicente, Xabier Saralegi, Ander Corral

Affiliation of the authors: 

Orai NLP Technologies



Licensing
-------------

Copyright (C) by Orai NLP Technologies. 
The corpora, datasets and models created in this work, are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
International License (CC BY-SA-NC 4.0). To view a copy of this license, visit [http://creativecommons.org/licenses/by-nc-sa/4.0/](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.eu).




Acknowledgements
-------------------
If you use this benchmark please cite the following paper:

- G. Urbizu, I. San Vicente, X. Saralegi, A. Corral. Not Enough Data to Pre-train Your Language Model? MT to the Rescue!. Findings of the Association for Computational Linguistics: ACL 2023. July, 2023. Toronto, Canada



Contact information
-----------------------
Gorka Urbizu, Iñaki San Vicente: {g.urbizu,i.sanvicente}@orai.eus
