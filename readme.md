## Not Enough Data to Pre-train Your Language Model? MT to the Rescue!

Data and models from our work [*Not Enough Data to Pre-train Your Language Model? MT to the Rescue!*](https://aclanthology.org/2023.findings-acl.235.pdf) accepted at ACL2023 Findings.


Authors
-----------
Gorka Urbizu, Iñaki San Vicente, Xabier Saralegi, Ander Corral

Orai NLP Technologies



Licensing
-------------

Copyright (C) by Orai NLP Technologies. 
The corpora, datasets and models created in this work, are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
International License (CC BY-NC-SA 4.0). To view a copy of this license, visit [http://creativecommons.org/licenses/by-nc-sa/4.0/](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.eu).




Acknowledgements
-------------------
If you use any of these models or datasets please cite the following paper:

- G. Urbizu, I. San Vicente, X. Saralegi, A. Corral. Not Enough Data to Pre-train Your Language Model? MT to the Rescue! Findings of the Association for Computational Linguistics: ACL 2023. July, 2023. Toronto, Canada

```
@inproceedings{urbizu2023not,
  title={Not Enough Data to Pre-train Your Language Model? MT to the Rescue!},
  author={Urbizu, Gorka and San Vicente, I{\~n}aki and Saralegi, Xabier and Corral, Ander},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={3826--3836},
  year={2023}
}
```

Contact information
-----------------------
Gorka Urbizu, Iñaki San Vicente: {g.urbizu,i.sanvicente}@orai.eus
